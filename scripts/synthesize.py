#!/usr/bin/env python3
"""
synthesize.py - LLM-assisted knowledge synthesis

CONTRACT:
    Input:  knowledge/chunks/<doc_name>/*.json
    Output: knowledge/synth/drafts/*.md

GUARANTEES:
    - LLM usage is allowed ONLY in this script
    - Drafts are clearly labeled as NON-AUTHORITATIVE
    - Script REFUSES to overwrite curated files
    - Every statement includes source citations
    - Missing/ambiguous info is explicitly stated
    - Human review is REQUIRED before promotion to curated status

CURATED FILES (NEVER OVERWRITTEN):
    - synth/glossary.md
    - synth/rules.md
    - synth/invariants.md
    - synth/contradictions.md
    - synth/open_questions.md
    - synth/procedures/*.md (any file without DRAFT prefix)

OUTPUT CATEGORIES:
    - drafts/DRAFT_glossary_<topic>.md
    - drafts/DRAFT_rules_<topic>.md
    - drafts/DRAFT_invariants_<topic>.md
    - drafts/DRAFT_procedures_<topic>.md
    - drafts/DRAFT_contradictions_<topic>.md
    - drafts/DRAFT_questions_<topic>.md

DEPENDENCIES:
    - anthropic: pip install anthropic (or openai for OpenAI)
"""

import argparse
import json
import os
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

# Project paths
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
CHUNKS_DIR = PROJECT_ROOT / "chunks"
SYNTH_DIR = PROJECT_ROOT / "synth"
DRAFTS_DIR = SYNTH_DIR / "drafts"
PROMPTS_DIR = PROJECT_ROOT / "prompts"

# Protected files that MUST NOT be overwritten
PROTECTED_FILES = [
    SYNTH_DIR / "glossary.md",
    SYNTH_DIR / "rules.md",
    SYNTH_DIR / "invariants.md",
    SYNTH_DIR / "contradictions.md",
    SYNTH_DIR / "open_questions.md",
]

DRAFT_HEADER = """
================================================================================
                              DRAFT - NOT AUTHORITATIVE
================================================================================

This document was generated by an LLM and has NOT been human-reviewed.

DO NOT treat this as truth. This is a DRAFT for human review.

To promote to curated status:
1. Review every statement
2. Verify every citation
3. Edit as needed
4. Move approved content to the appropriate curated file

Generated: {timestamp}
Source chunks: {chunk_count}
================================================================================

"""


def check_protected_files(output_path: Path) -> bool:
    """
    Check if output path would overwrite a protected file.

    Returns True if safe, raises error if protected.
    """
    output_path = output_path.resolve()

    for protected in PROTECTED_FILES:
        if output_path == protected.resolve():
            raise ValueError(
                f"REFUSED: Cannot overwrite protected curated file: {protected}\n"
                f"Curated files require human review. Write to drafts/ instead."
            )

    # Also check procedures/ for non-draft files
    procedures_dir = SYNTH_DIR / "procedures"
    if procedures_dir in output_path.parents:
        if not output_path.name.startswith("DRAFT_"):
            raise ValueError(
                f"REFUSED: Cannot overwrite curated procedure: {output_path}\n"
                f"Use DRAFT_ prefix for new procedures."
            )

    return True


def load_prompt(prompt_name: str) -> str:
    """Load a prompt template from prompts/ directory."""
    prompt_file = PROMPTS_DIR / f"{prompt_name}.md"
    if not prompt_file.exists():
        raise FileNotFoundError(f"Prompt not found: {prompt_file}")
    return prompt_file.read_text(encoding="utf-8")


def load_chunks(chunks_dir: Path, limit: Optional[int] = None) -> list[dict]:
    """Load chunks from all documents (recursive)."""
    chunks = []

    # Use recursive glob to find all chunk JSON files
    for chunk_file in sorted(chunks_dir.rglob("*.json")):
        if chunk_file.name.startswith("_"):
            continue  # Skip metadata files

        try:
            chunk = json.loads(chunk_file.read_text(encoding="utf-8"))
            chunks.append(chunk)
        except (json.JSONDecodeError, IOError) as e:
            print(f"Warning: Could not load {chunk_file}: {e}", file=sys.stderr)

        if limit and len(chunks) >= limit:
            return chunks

    return chunks


def format_chunks_for_prompt(chunks: list[dict]) -> str:
    """Format chunks as context for LLM prompt."""
    formatted = []

    for chunk in chunks:
        chunk_text = f"""
--- CHUNK: {chunk.get('id', 'unknown')} ---
Source: {chunk.get('source_document', 'unknown')}
Section: {chunk.get('section', 'N/A')}
Pages: {chunk.get('page_start', '?')}-{chunk.get('page_end', '?')}

{chunk.get('raw_text', '')}
--- END CHUNK ---
"""
        formatted.append(chunk_text)

    return "\n".join(formatted)


def call_llm(prompt: str, context: str, api_key: Optional[str] = None) -> str:
    """
    Call LLM API to generate synthesis.

    Supports Anthropic Claude API. Can be extended for other providers.
    """
    # Try to get API key from environment if not provided
    api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")

    if not api_key:
        raise ValueError(
            "No API key provided. Set ANTHROPIC_API_KEY environment variable "
            "or use --api-key argument."
        )

    try:
        import anthropic
    except ImportError:
        raise ImportError(
            "anthropic package not installed. Run: pip install anthropic"
        )

    client = anthropic.Anthropic(api_key=api_key)

    full_prompt = f"""You are a knowledge extraction assistant. Your task is to synthesize knowledge from source documents.

CRITICAL RULES:
1. NEVER infer beyond what the text explicitly states
2. ALWAYS cite the source chunk ID, document, and page for every statement
3. If information is missing or ambiguous, say "NOT SPECIFIED IN SOURCE"
4. Do not fill gaps with assumptions
5. Every factual claim must have a citation

{prompt}

SOURCE CHUNKS:
{context}

Generate the synthesis following the format specified above. Remember: citation is mandatory for every statement.
"""

    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=4096,
        messages=[
            {"role": "user", "content": full_prompt}
        ]
    )

    return message.content[0].text


def generate_draft(
    synthesis_type: str,
    topic: str,
    chunks: list[dict],
    api_key: Optional[str] = None,
    verbose: bool = False,
    full: bool = False,
) -> Path:
    """
    Generate a synthesis document.

    Args:
        full: If True, output to synth/<type>.md; if False, output to drafts/DRAFT_*.md

    Returns path to generated document.
    """
    # Load appropriate prompt
    prompt = load_prompt(f"synthesize_{synthesis_type}")

    # Format chunks as context
    context = format_chunks_for_prompt(chunks)

    if verbose:
        print(f"  Calling LLM with {len(chunks)} chunks...")

    # Call LLM
    synthesis = call_llm(prompt, context, api_key)

    # Determine output path based on full flag
    if full:
        # Output to curated file
        output_path = SYNTH_DIR / f"{synthesis_type}.md"
        # Create header for full document (still warns it's LLM-generated)
        header = f"""# {synthesis_type.capitalize()}

> **WARNING**: This document was generated by an LLM using {len(chunks)} chunks.
> It requires human review and verification before being considered authoritative.
>
> **Generated**: {datetime.now(timezone.utc).isoformat()}
> **Source**: {len(chunks)} chunks from knowledge base
> **Status**: Awaiting human review and validation

---

"""
    else:
        # Output to drafts
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        safe_topic = topic.replace(" ", "_").replace("/", "_")[:50]
        draft_filename = f"DRAFT_{synthesis_type}_{safe_topic}_{timestamp}.md"
        output_path = DRAFTS_DIR / draft_filename

        # Add draft header
        header = DRAFT_HEADER.format(
            timestamp=datetime.now(timezone.utc).isoformat(),
            chunk_count=len(chunks),
        )

    # Ensure we're not overwriting protected files (only check for drafts)
    if not full:
        check_protected_files(output_path)

    full_content = header + synthesis

    # Write output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(full_content, encoding="utf-8")

    return output_path


def main():
    parser = argparse.ArgumentParser(
        description="Generate synthesis drafts using LLM (requires human review)"
    )
    parser.add_argument(
        "type",
        choices=["glossary", "rules", "invariants", "procedures", "contradictions", "questions"],
        help="Type of synthesis to generate",
    )
    parser.add_argument(
        "--topic",
        type=str,
        required=True,
        help="Topic or focus area for synthesis",
    )
    parser.add_argument(
        "--chunks",
        type=Path,
        default=CHUNKS_DIR,
        help=f"Input directory with chunks (default: {CHUNKS_DIR})",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=50,
        help="Maximum number of chunks to process (default: 50)",
    )
    parser.add_argument(
        "--api-key",
        type=str,
        help="Anthropic API key (or set ANTHROPIC_API_KEY env var)",
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Print detailed progress",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be done without calling LLM",
    )
    parser.add_argument(
        "--full",
        action="store_true",
        help="Output to curated file (synth/<type>.md) instead of drafts/",
    )

    args = parser.parse_args()

    print("=" * 60)
    print("SYNTHESIS - LLM-GENERATED DRAFTS")
    print("=" * 60)
    print()
    print("WARNING: This script uses an LLM to generate drafts.")
    print("Drafts are NOT authoritative and require human review.")
    print()

    # Load chunks
    print(f"Loading chunks from: {args.chunks}")
    chunks = load_chunks(args.chunks, limit=args.limit)

    if not chunks:
        print(f"ERROR: No chunks found in {args.chunks}", file=sys.stderr)
        sys.exit(1)

    print(f"Loaded {len(chunks)} chunks")
    print(f"Synthesis type: {args.type}")
    print(f"Topic: {args.topic}")
    print()

    if args.dry_run:
        print("[DRY RUN] Would generate draft with above parameters")
        print("[DRY RUN] No LLM call made, no files written")
        sys.exit(0)

    # Check for prompt file
    prompt_file = PROMPTS_DIR / f"synthesize_{args.type}.md"
    if not prompt_file.exists():
        print(f"ERROR: Prompt file not found: {prompt_file}", file=sys.stderr)
        print(f"Please create the prompt file first.", file=sys.stderr)
        sys.exit(1)

    # Generate synthesis
    output_type = "full document" if args.full else "draft"
    print(f"Generating synthesis {output_type}...")
    try:
        output_path = generate_draft(
            args.type,
            args.topic,
            chunks,
            api_key=args.api_key,
            verbose=args.verbose,
            full=args.full,
        )
    except ValueError as e:
        print(f"ERROR: {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"ERROR: LLM call failed: {e}", file=sys.stderr)
        sys.exit(1)

    print()
    if args.full:
        print(f"Full document generated: {output_path}")
        print()
        print("=" * 60)
        print("NEXT STEPS (REQUIRED):")
        print("=" * 60)
        print("1. Review the generated document carefully")
        print("2. Verify every citation against source documents")
        print("3. Edit for accuracy and completeness")
        print("4. Validate all claims and remove any hallucinations")
        print("5. Add human expertise and context where needed")
        print()
        print("[!] WARNING: This is LLM-generated content.")
        print("    Human review is MANDATORY before considering it authoritative.")
        print("    The LLM is stateless and has no memory.")
        print("=" * 60)
    else:
        print(f"Draft generated: {output_path}")
        print()
        print("=" * 60)
        print("NEXT STEPS (REQUIRED):")
        print("=" * 60)
        print("1. Review the generated draft carefully")
        print("2. Verify every citation against source documents")
        print("3. Edit for accuracy and completeness")
        print("4. Move approved content to the appropriate curated file:")
        print(f"   - synth/{args.type}.md (or synth/procedures/)")
        print("5. Delete the draft file after promotion")
        print()
        print("The LLM is stateless. It has no memory.")
        print("Human review is MANDATORY before this becomes knowledge.")
        print("=" * 60)


if __name__ == "__main__":
    main()
